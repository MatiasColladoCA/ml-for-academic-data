version: '3.8'


networks:
  internal_network:
    driver: bridge

services:
  airflow:
  # En caso de falta de contraseña (ver al iniciar sesión en la pagina autohosteada) abrir terminal de contenedor airflow, abrir archivo config y buscar "password": docker ps, docker exec -it <nombre_del_contenedor> bash, /opt/airflow/airflow.cfg, cat /opt/airflow/airflow.cfg
  
    build:
      context: ./airflow-w-sklearn    # <-- Aquí apunta al Dockerfile de la carpeta dedicada
    # image: apache/airflow:2.9.2
    image: airflow-w-sklearn
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor #No apto para producción
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__FERNET__KEY=D_b7-k2tJcEmrfAPASL8V_cY03LJd-uWWtJ-8HN1Esk=
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      # usuario por defecto
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
      # # para que todas las carpetas de src/ sean importables
      # - PYTHONPATH=/opt/airflow/src

    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
      - ./volumes/airflow:/opt/airflow
    
    ports:
      - "8080:8080"
    # command: standalone
    command: >
      bash -c "airflow db init &&
                airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
                airflow webserver"


  airflow-scheduler:
    build:
      context: ./airflow-w-sklearn    # <-- Aquí apunta al Dockerfile de la carpeta dedicada
    image: airflow-w-sklearn
    container_name: airflow-scheduler
    command: airflow scheduler
    depends_on:
      - airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
      - ./volumes/airflow:/opt/airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor


  keycloak:
    image: quay.io/keycloak/keycloak:22.0.5
    container_name: keycloak
    environment:
      - KEYCLOAK_ADMIN=admin
      - KEYCLOAK_ADMIN_PASSWORD=admin
      - DB_VENDOR=h2
    volumes:
      - ./volumes/keycloak:/opt/keycloak/data
    ports:
      - "8081:8080"
    command:
      - start-dev


  oauth2-proxy:
    image: bitnami/oauth2-proxy:latest
    container_name: oauth2-proxy
    environment:
      - OAUTH2_PROXY_CLIENT_ID=your-client-id
      - OAUTH2_PROXY_CLIENT_SECRET=your-client-secret
      - OAUTH2_PROXY_COOKIE_SECRET=Zm9vYmFyMTIzYWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXowMTI=
      - OAUTH2_PROXY_PROVIDER_OIDC_ISSUER_URL=http://localhost:8081/realms/your-realm
      - OAUTH2_PROXY_EMAIL_DOMAINS=*
      - OAUTH2_PROXY_HTTP_ADDRESS=0.0.0.0:4180
      - OAUTH2_PROXY_UPSTREAM=http://host.docker.internal:5006
    ports:
      - "4180:4180"

  # airflow-init:
  #   image: apache/airflow:2.9.2
  #   command: /bin/bash -c "pip install --no-cache-dir -r /requirements.txt"
  #   volumes:
  #     - .:/requirements.txt:/requirements.txt
  #   environment:
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=false

    # docker-compose.yml

  mysql-server:
    image: mysql:8.0
    container_name: mysql-server
    environment:
      MYSQL_ROOT_PASSWORD: rootpass
      MYSQL_DATABASE: students
      MYSQL_USER: user
      MYSQL_PASSWORD: 1234
    ports:
      - "3306:3306" # Opcional: solo si necesitas acceder desde fuera del contenedor
    volumes:
      - .volumes/mysql_data:/var/lib/mysql
      - .volumes/mysql_data/init.sql:/docker-entrypoint-initdb.d/init.sql  # script inicial (opcional)
    networks:
      - internal_network

  # etl-app:
  #   image: python:3.10-slim
  #   container_name: etl-app
  #   working_dir: /app
  #   volumes:
  #     - .:/app
  #   depends_on:
  #     - mysql-server
  #   networks:
  #     - internal_network
  #   command: tail -f /dev/null  # Mantiene el contenedor corriendo para pruebas